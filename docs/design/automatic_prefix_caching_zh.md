---
title：自动前缀缓存
---
[](){ #design-automatic-prefix-caching }

[PagedAttention](https://blog.vllm.ai/2023/06/20/vllm.html) 的核心思想是将每个请求的键值缓存划分为多个键值块。每个块包含固定数量 token 的关注键和值。PagedAttention 算法允许这些块存储在非连续的物理内存中，以便我们通过按需分配内存来消除内存碎片。

为了自动缓存键值缓存，我们利用了以下关键观察：每个键值块都可以通过块内的 token 和块前前缀中的 token 唯一标识。

```text
块 1 块 2 块 3
[微风拂过] [树叶像孩子们] [远方传来欢笑]
块 1：|<--- 块 token ---->|
块 2：|<------- 前缀 ------>| |<--- 块标记 --->|
块 3：|<------------------ 前缀 -------------------->| |<--- 块标记 ---->|
```

在上面的例子中，第一个块中的键值缓存可以用标记“A gentle breeze stird”（微风轻拂）唯一标识。第三个块可以用块“laughed in the distance”（在远处笑）中的标记以及前缀标记“A gentle breeze stird the leaves as children”（微风轻拂树叶如同孩童）唯一标识。因此，我们可以构建以下一对一映射：

```text
hash(前缀标记 + 块标记) <--> 键值块
```

通过此映射，我们可以在 vLLM 的键值缓存管理中添加另一个间接机制。之前，vLLM 中的每个序列都维护着从其逻辑键值块到物理块的映射。为了实现键值块的自动缓存，我们将逻辑键值块映射到其哈希值，并维护一个包含所有物理块的全局哈希表。这样，所有共享相同哈希值的键值块（例如，两个请求之间共享的前缀块）都可以映射到同一个物理块并共享内存空间。

这种设计实现了自动前缀缓存，无需在键值块之间维护树形结构。更具体地说，所有块彼此独立，可以自行分配和释放，这使得我们可以像管理操作系统中的普通缓存一样管理键值缓存。

## 通用缓存策略

将所有键值块保存在哈希表中，使 vLLM 能够缓存早期请求的键值块，从而节省内存并加速未来请求的计算。例如，如果新请求与前一个请求共享系统提示符，则共享提示符的键值缓存可以直接用于新请求，而无需重新计算。然而，总的键值缓存空间是有限的，当缓存已满时，我们必须决定保留或移除哪些键值块。

使用哈希表管理键值缓存使我们能够实现灵活的缓存策略。例如，在当前的 vLLM 中，我们实现了以下移除策略：

* 当没有剩余空闲块时，我们将移除引用计数（即当前使用该块的请求数）为 0 的键值块。
* 如果有多个引用计数为 0 的块，我们将优先移除最近最少使用的块 (LRU)。
* 如果有多个块的最后访问时间相同，我们将优先移除位于最长前缀末尾的块（即在其之前拥有最多块数的块）。

请注意，此驱逐策略在应用于具有完全注意力机制的模型时，其有效实现的策略与 [RadixAttention](https://lmsys.org/blog/2024-01-17-sglang/) 中的策略完全相同，即优先驱逐前缀树中引用计数为零和最近最少使用的叶节点。

然而，基于哈希的键值缓存管理使我们能够灵活地处理更复杂的服务场景，并实现比上述策略更复杂的驱逐策略：

* 多 LoRA 服务。当服务于多个 LoRA 适配器的请求时，我们可以简单地让每个键值块的哈希值也包含请求所查询的 LoRA ID，从而为所有适配器启用缓存。通过这种方式，我们可以联合管理不同适配器的键值块，从而简化系统实现并提高全局缓存命中率和效率。
* 多模态模型。当用户输入不仅仅包含离散的 token 时，我们可以使用不同的哈希方法来处理不同模态输入的缓存。例如，对图像进行感知散列以缓存类似的输入图像。