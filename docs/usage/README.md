# 使用 vLLM

vLLM 支持以下使用模式：

- [推理和服务](../serving/offline_inference.md)：运行模型的单个实例。
- [部署](../deployment/docker.md)：扩展模型实例以用于生产环境。
- [训练](../training/rlhf.md)：训练或微调模型。