# --8<-- [start:installation]

vLLM 最初支持在 x86 CPU 平台上进行基本模型推理和服务，数据类型为 FP32、FP16 和 BF16。

!!! 警告
此设备没有预构建的 Wheel 或镜像，因此您必须从源代码构建 vLLM。

# --8<-- [end:installation]
# --8<-- [start:requirements]

- 操作系统：Linux
- 编译器：`gcc/g++ >= 12.3.0`（可选，推荐）
- 指令集架构 (ISA)：AVX512（可选，推荐）

!!! 提示
[Intel PyTorch 扩展 (IPEX)](https://github.com/intel/intel-extension-for-pytorch) 扩展了 PyTorch，并对其进行了最新的功能优化，以在 Intel 硬件上进一步提升性能。

# --8<-- [end:requirements]
# --8<-- [start:set-up-using-python]

# --8<-- [end:set-up-using-python]
# --8<-- [start:pre-built-wheels]

# --8<-- [end:pre-built-wheels]
# --8<-- [start:build-wheel-from-source]

--8<-- "docs/getting_started/installation/cpu/cpu/build.inc.md"

!!! 注意
- AVX512_BF16 是一个扩展指令集 (ISA)，提供原生的 BF16 数据类型转换和矢量积指令，与纯 AVX512 指令集相比，它带来了一些性能提升。CPU 后端构建脚本将检查主机 CPU 标志，以确定是否启用 AVX512_BF16。
- 如果您想强制启用 AVX512_BF16 进行交叉编译，请在构建之前设置环境变量“VLLM_CPU_AVX512BF16=1”。

# --8<-- [end:build-wheel-from-source]
# --8<-- [start:set-up-using-docker]

# --8<-- [end:set-up-using-docker]
# --8<-- [start:pre-built-images]

请参阅 [https://gallery.ecr.aws/q9t5s3a7/vllm-cpu-release-repo](https://gallery.ecr.aws/q9t5s3a7/vllm-cpu-release-repo)

# --8<-- [end:pre-built-images]
# --8<-- [start:build-image-from-source]

# --8<-- [end:build-image-from-source]
# --8<-- [start:extra-information]
# --8<-- [end:extra-information]