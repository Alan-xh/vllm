<p align="center">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-dark.png">
    <img alt="vLLM" src="https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-light.png" width=55%>
  </picture>
</p>

<h3 align="center">
简单、快速、经济的 LLM 服务，适合所有人
</h3>

<p align="center">
| <a href="https://docs.vllm.ai"><b>文档</b></a> | <a href="https://blog.vllm.ai/"><b>博客</b></a> | <a href="https://arxiv.org/abs/2309.06180"><b>论文</b></a> | <a href="https://x.com/vllm_project"><b>Twitter/X</b></a> | <a href="https://discuss.vllm.ai"><b>用户论坛</b></a> | <a href="https://slack.vllm.ai"><b>开发者 Slack</b></a> |
</p>

---

*最新动态* 🔥
- [2025/05] 我们举办了 [纽约 vLLM 聚会](https://lu.ma/c1rqyf1f)！请查看聚会幻灯片 [此处](https://docs.google.com/presentation/d/1_q_aW_ioMJWUImf1s1YM-ZhjXz8cUeL0IJvaquOYBeA/edit?usp=sharing)。
- [2025/05] vLLM 现已成为 PyTorch 基金会托管项目！请查看公告 [此处](https://pytorch.org/blog/pytorch-foundation-welcomes-vllm/)。
- [2025/04] 我们举办了 [亚洲开发者日](https://www.sginnovate.com/event/limited-availability-morning-evening-slots-remaining-inaugural-vllm-asia-developer-day)！请查看 vLLM 团队的聚会幻灯片 [此处](https://docs.google.com/presentation/d/19cp6Qu8u48ihB91A064XfaXruNYiBOUKrBxAmDOllOo/edit?usp=sharing)。
- [2025/01] 我们很高兴宣布 vLLM V1 Alpha 版发布：重大架构升级，速度提升 1.7 倍！代码更简洁，优化执行循环，零开销前缀缓存，增强的多模态支持等。请查看我们的博客文章 [此处](https://blog.vllm.ai/2025/01/27/v1-alpha-release.html)。

<details>
<summary>往期动态</summary>

- [2025/03] 我们举办了 [vLLM x Ollama 推理之夜](https://lu.ma/vllm-ollama)！请查看 vLLM 团队的聚会幻灯片 [此处](https://docs.google.com/presentation/d/16T2PDD1YwRnZ4Tu8Q5r6n53c5Lr5c73UV9Vd2_eBo4U/edit?usp=sharing)。
- [2025/03] 我们举办了 [首届 vLLM 中国聚会](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg)！请查看 vLLM 团队的聚会幻灯片 [此处](https://docs.google.com/presentation/d/1REHvfQMKGnvz6p3Fd23HhSO4c8j5WPGZV0bKYLwnHyQ/edit?usp=sharing)。
- [2025/03] 我们举办了 [东海岸 vLLM 聚会](https://lu.ma/7mu4k4xx)！请查看聚会幻灯片 [此处](https://docs.google.com/presentation/d/1NHiv8EUFF1NLd3fEYODm56nDmL26lEeXCaDgyDlTsRs/edit#slide=id.g31441846c39_0_0)。
- [2025/02] 我们与 Meta 共同举办了 [第九次 vLLM 聚会](https://lu.ma/h7g3kuj9)！请查看 vLLM 团队的聚会幻灯片 [此处](https://docs.google.com/presentation/d/1jzC_PZVXrVNSFVCW-V4cFXb6pn7zZ2CyP_Flwo05aqg/edit?usp=sharing) 和 AMD 的幻灯片 [此处](https://drive.google.com/file/d/1Zk5qEJIkTmlQ2eQcXQZlljAx3m9s7nwn/view?usp=sharing)。Meta 的幻灯片不会公开。
- [2025/01] 我们与 Google Cloud 共同举办了 [第八次 vLLM 聚会](https://lu.ma/zep56hui)！请查看 vLLM 团队的聚会幻灯片 [此处](https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing) 和 Google Cloud 团队的幻灯片 [此处](https://drive.google.com/file/d/1h24pHewANyRL11xy5dXUbvRC9F9Kkjix/view?usp=sharing)。
- [2024/12] vLLM 加入 [PyTorch 生态系统](https://pytorch.org/blog/vllm-joins-pytorch)！简单、快速、经济的 LLM 服务，适合所有人！
- [2024/11] 我们与 Snowflake 共同举办了 [第七次 vLLM 聚会](https://lu.ma/h0qvrajz)！请查看 vLLM 团队的聚会幻灯片 [此处](https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing) 和 Snowflake 团队的幻灯片 [此处](https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing)。
- [2024/10] 我们创建了开发者 Slack ([slack.vllm.ai](https://slack.vllm.ai))，专注于协调贡献和讨论功能。请随时加入我们！
- [2024/10] Ray Summit 2024 为 vLLM 举办了特别专题！请查看 vLLM 团队的开幕演讲幻灯片 [此处](https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing)。了解更多其他 vLLM 贡献者和用户的 [演讲](https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR)！
- [2024/09] 我们与 NVIDIA 共同举办了 [第六次 vLLM 聚会](https://lu.ma/87q3nvnh)！请查看聚会幻灯片 [此处](https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing)。
- [2024/07] 我们与 AWS 共同举办了 [第五次 vLLM 聚会](https://lu.ma/lp0gyjqr)！请查看聚会幻灯片 [此处](https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing)。
- [2024/07] 与 Meta 合作，vLLM 正式支持 Llama 3.1，包含 FP8 量化和流水线并行！请查看我们的博客文章 [此处](https://blog.vllm.ai/2024/07/23/llama31.html)。
- [2024/06] 我们与 Cloudflare 和 BentoML 共同举办了 [第四次 vLLM 聚会](https://lu.ma/agivllm)！请查看聚会幻灯片 [此处](https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing)。
- [2024/04] 我们与 Roblox 共同举办了 [第三次 vLLM 聚会](https://robloxandvllmmeetup2024.splashthat.com/)！请查看聚会幻灯片 [此处](https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing)。
- [2024/01] 我们与 IBM 共同举办了 [第二次 vLLM 聚会](https://lu.ma/ygxbpzhl)！请查看聚会幻灯片 [此处](https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing)。
- [2023/10] 我们与 a16z 共同举办了 [第一次 vLLM 聚会](https://lu.ma/first-vllm-meetup)！请查看聚会幻灯片 [此处](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing)。
- [2023/08] 我们衷心感谢 [Andreessen Horowitz](https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/) (a16z) 提供慷慨资助，支持 vLLM 的开源开发和研究。
- [2023/06] 我们正式发布 vLLM！FastChat-vLLM 集成自 4 月中旬以来为 [LMSYS Vicuna 和 Chatbot Arena](https://chat.lmsys.org) 提供支持。查看我们的 [博客文章](https://vllm.ai)。

</details>

---
## 关于

vLLM 是一个快速且易于使用的 LLM 推理和服务库。

最初由 [加州大学伯克利分校 Sky Computing Lab](https://sky.cs.berkeley.edu) 开发，vLLM 已演变为一个社区驱动的项目，汇集了学术界和工业界的贡献。

vLLM 的特点是快速：

- 最先进的吞吐量
- 使用 [**PagedAttention**](https://blog.vllm.ai/2023/06/20/vllm.html) 高效管理注意力键和值内存
- 持续批处理传入请求
- 使用 CUDA/HIP 图进行快速模型执行
- 量化支持：[GPTQ](https://arxiv.org/abs/2210.17323)、[AWQ](https://arxiv.org/abs/2306.00978)、[AutoRound](https://arxiv.org/abs/2309.05516)、INT4、INT8 和 FP8
- 优化的 CUDA 内核，集成 FlashAttention 和 FlashInfer
- 推测解码
- 分块预填充

**性能基准**：我们在 [博客文章](https://blog.vllm.ai/2024/09/05/perf-update.html) 末尾提供了性能基准，比较了 vLLM 与其他 LLM 服务引擎（[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)、[SGLang](https://github.com/sgl-project/sglang) 和 [LMDeploy](https://github.com/InternLM/lmdeploy)）的性能。实现代码位于 [nightly-benchmarks 文件夹](.buildkite/nightly-benchmarks/)，您可以使用我们的一键运行脚本 [重现](https://github.com/vllm-project/vllm/issues/8176) 该基准。

vLLM 灵活且易于使用，具有以下特点：

- 与热门 Hugging Face 模型无缝集成
- 支持多种解码算法的高吞吐量服务，包括 *并行采样*、*束搜索* 等
- 支持张量并行和流水线并行进行分布式推理
- 流式输出
- 兼容 OpenAI 的 API 服务器
- 支持 NVIDIA GPU、AMD CPU 和 GPU、Intel CPU 和 GPU、PowerPC CPU、TPU 和 AWS Neuron
- 支持前缀缓存
- 支持多 LoRA

vLLM 无缝支持 HuggingFace 上大多数流行的开源模型，包括：
- 类 Transformer LLM（如 Llama）
- 混合专家模型（如 Mixtral、Deepseek-V2 和 V3）
- 嵌入模型（如 E5-Mistral）
- 多模态 LLM（如 LLaVA）

查看支持的模型完整列表 [此处](https://docs.vllm.ai/en/latest/models/supported_models.html)。

## 入门

使用 `pip` 安装 vLLM 或 [从源码](https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source) 安装：

```bash
pip install vllm
```

访问我们的 [文档](https://docs.vllm.ai/en/latest/) 了解更多：
- [安装](https://docs.vllm.ai/en/latest/getting_started/installation.html)
- [快速入门](https://docs.vllm.ai/en/latest/getting_started/quickstart.html)
- [支持的模型列表](https://docs.vllm.ai/en/latest/models/supported_models.html)

## 贡献

我们欢迎并重视任何贡献和合作。
请查看 [贡献 vLLM](https://docs.vllm.ai/en/latest/contributing/index.html) 了解如何参与。

## 赞助商

vLLM 是一个社区项目。我们的开发和测试计算资源由以下组织支持。感谢您的支持！

<!-- 注意：请按字母顺序排列。 -->
<!-- 注意：请与 docs/community/sponsors.md 保持一致 -->
现金捐助：
- a16z
- Dropbox
- Sequoia Capital
- Skywork AI
- ZhenFund

计算资源：
- AMD
- Anyscale
- AWS
- Crusoe Cloud
- Databricks
- DeepInfra
- Google Cloud
- Intel
- Lambda Lab
- Nebius
- Novita AI
- NVIDIA
- Replicate
- Roblox
- RunPod
- Trainy
- UC Berkeley
- UC San Diego

Slack 赞助商：Anyscale

我们还通过 [OpenCollective](https://opencollective.com/vllm) 建立了官方筹资渠道。我们计划使用这些资金支持 vLLM 的开发、维护和推广。

## 引用

如果您在研究中使用 vLLM，请引用我们的 [论文](https://arxiv.org/abs/2309.06180)：

```bibtex
@inproceedings{kwon2023efficient,
  title={使用 PagedAttention 进行大型语言模型服务的高效内存管理},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={ACM SIGOPS 第29届操作系统原理研讨会论文集},
  year={2023}
}
```

## 联系我们

- 对于技术问题和功能请求，请使用 GitHub [Issues](https://github.com/vllm-project/vllm/issues) 或 [Discussions](https://github.com/vllm-project/vllm/discussions)
- 与其他用户交流，请使用 [vLLM 论坛](https://discuss.vllm.ai)
- 协调贡献和开发，请使用 [Slack](https://slack.vllm.ai)
- 对于安全披露，请使用 GitHub 的 [安全咨询](https://github.com/vllm-project/vllm/security/advisories) 功能
- 对于合作和伙伴关系，请联系我们： [vllm-questions@lists.berkeley.edu](mailto:vllm-questions@lists.berkeley.edu)

## 媒体资源

- 如果您希望使用 vLLM 的标志，请参考 [我们的媒体资源库](https://github.com/vllm-project/media-kit)